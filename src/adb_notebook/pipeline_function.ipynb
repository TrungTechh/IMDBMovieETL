{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7527e85-eca8-42fc-9a14-2048bf88181d",
     "showTitle": false,
     "title": ""
    },
    "id": "1xtiySa8w4Vh"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, to_date, split, regexp_extract, when, udf, lit, regexp_replace, cast, substr\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType, DateType, StructType, StructField, StringType, ArrayType, LongType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a38907-fd75-436a-8414-2edfba18e219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mount container silver in moviescrapingsa into Databrick successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mount_container(container_name, mount_folder, storage_account_name, client_id, directory_id, key):\n",
    "  if any(mount.mountPoint == mount_folder for mount in dbutils.fs.mounts()):\n",
    "    print(f\"Container '{container_name}' is already mounted.\")\n",
    "    return False\n",
    "  else:\n",
    "    configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "    \"fs.azure.account.oauth2.client.secret\": key,\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"}\n",
    "\n",
    "\n",
    "    dbutils.fs.mount(\n",
    "    source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net\", # contrainer@storageacc\n",
    "    mount_point = mount_folder,\n",
    "    extra_configs = configs)\n",
    "    print(f\"Mount container {container_name} in {storage_account_name} into Databrick successfully\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44bcaf1-f4f9-4888-9701-3dbee3e6d047",
     "showTitle": false,
     "title": ""
    },
    "id": "KVG72QYoxFrK"
   },
   "outputs": [],
   "source": [
    "def create_empty_delta_table(table_path,schema):\n",
    "    empty_df = spark.createDataFrame([], schema=schema)\n",
    "    empty_df.write.format(\"delta\").mode(\"ignore\").save(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47334f45-da55-4654-995c-cdcda5406286",
     "showTitle": false,
     "title": ""
    },
    "id": "rc-DY9hjtro7"
   },
   "outputs": [],
   "source": [
    "class BronzeBDS():\n",
    "    def __init__(self,read_path,write_path,checkpoint_path):\n",
    "        self.read_path = read_path\n",
    "        self.write_path = write_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    def get_schema(self):\n",
    "        schema = StructType([\n",
    "          StructField(\"name\", StringType(), True),\n",
    "          StructField(\"release_date\", StringType(), True),\n",
    "          StructField(\"genre\", StringType(), True),\n",
    "          StructField(\"certificate\", StringType(), True),\n",
    "          StructField(\"vote_count\", StringType(), True),\n",
    "          StructField(\"runtime\", StringType(), True),\n",
    "          StructField(\"imdb_score\", StringType(), True),\n",
    "          StructField(\"director\", StringType(), True),\n",
    "          StructField(\"writter\", StringType(), True),\n",
    "          StructField(\"stars\", StringType(), True),\n",
    "          StructField(\"budget\", StringType(), True),\n",
    "          StructField(\"gross_global\", StringType(), True),\n",
    "          StructField(\"countries\", StringType(), True),\n",
    "          StructField(\"language\", StringType(), True),\n",
    "          StructField(\"locations\", StringType(), True),\n",
    "          StructField(\"company\", StringType(), True),\n",
    "          StructField(\"url\", StringType(), True),\n",
    "      ])\n",
    "        return schema\n",
    "\n",
    "    def get_raw_data(self):\n",
    "        lines = (spark.readStream\n",
    "                    .format('json')\n",
    "                    .option(\"multiline\", \"true\")\n",
    "                    # .option(\"maxFilesPerTrigger\", 1)\n",
    "                    .schema(self.get_schema())\n",
    "                    .load(f\"{self.read_path}\")\n",
    "                )\n",
    "\n",
    "        return lines\n",
    "\n",
    "\n",
    "    def append_bronze_data(self,bronze_df,trigger):\n",
    "        sQuery =  ( bronze_df.writeStream\n",
    "                            .format('delta')\n",
    "                            .queryName(\"bronze-ingestion\")\n",
    "                            .option(\"checkpointLocation\",f\"{self.checkpoint_path}/bronze_table_checkpoint\")\n",
    "                            .outputMode(\"append\")\n",
    "        )\n",
    "\n",
    "        if(trigger=='batch'):\n",
    "            return ( sQuery.trigger(availableNow = True)\n",
    "                         .start(f\"{self.write_path}/bronze_delta_table\"))\n",
    "        else:\n",
    "            return ( sQuery.trigger(processingTime = trigger)\n",
    "                         .start(f\"{self.write_path}/bronze_delta_table\"))\n",
    "\n",
    "\n",
    "    def process(self,trigger='batch'):\n",
    "        print(f\"\\nStarting Bronze Stream...\")\n",
    "        # dbutils.fs.mkdirs(self.read_path)\n",
    "        # Read\n",
    "        bronze_df = self.get_raw_data()\n",
    "\n",
    "        # Transform\n",
    "        #create_empty_delta_table(f\"{self.write_path}/bronze_delta_table\",bronze_df.schema)\n",
    "\n",
    "        # Write\n",
    "        sQuery =  self.append_bronze_data(bronze_df,trigger)\n",
    "        return sQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e3c1c2e-2b11-46c1-9712-767fd5cdf68d",
     "showTitle": false,
     "title": ""
    },
    "id": "187ebHAtzHwr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-3841870613545280>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSilverBDS\u001b[39;00m():\n",
       "\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,read_path,write_path,checkpoint_path):\n",
       "\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_path \u001b[38;5;241m=\u001b[39m read_path\n",
       "\n",
       "File \u001b[0;32m<command-3841870613545280>, line 8\u001b[0m, in \u001b[0;36mSilverBDS\u001b[0;34m()\u001b[0m\n",
       "\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_path \u001b[38;5;241m=\u001b[39m write_path\n",
       "\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_path \u001b[38;5;241m=\u001b[39m checkpoint_path\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n",
       "\u001b[0;32m----> 8\u001b[0m \u001b[38;5;129m@udf\u001b[39m(\u001b[43mIntegerType\u001b[49m())\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Convert runtime\u001b[39;00m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_minutes\u001b[39m(runtime):\n",
       "\u001b[1;32m     11\u001b[0m     hours \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
       "\u001b[1;32m     12\u001b[0m     minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
       "\n",
       "\u001b[0;31mNameError\u001b[0m: name 'IntegerType' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\nFile \u001b[0;32m<command-3841870613545280>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSilverBDS\u001b[39;00m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,read_path,write_path,checkpoint_path):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_path \u001b[38;5;241m=\u001b[39m read_path\n\nFile \u001b[0;32m<command-3841870613545280>, line 8\u001b[0m, in \u001b[0;36mSilverBDS\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_path \u001b[38;5;241m=\u001b[39m write_path\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_path \u001b[38;5;241m=\u001b[39m checkpoint_path\n\u001b[1;32m      7\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;129m@udf\u001b[39m(\u001b[43mIntegerType\u001b[49m())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#Convert runtime\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_minutes\u001b[39m(runtime):\n\u001b[1;32m     11\u001b[0m     hours \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m     minutes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\n\u001b[0;31mNameError\u001b[0m: name 'IntegerType' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'IntegerType' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SilverBDS():\n",
    "    def __init__(self,read_path,write_path,checkpoint_path):\n",
    "        self.read_path = read_path\n",
    "        self.write_path = write_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "\n",
    "    @staticmethod\n",
    "    @udf(IntegerType())\n",
    "    #Convert runtime\n",
    "    def convert_to_minutes(runtime):\n",
    "        hours = 0\n",
    "        minutes = 0\n",
    "        if runtime is None:\n",
    "          return None\n",
    "        if \"hour\" in runtime:\n",
    "            hours = int(re.search(r'(\\d+) hour', runtime).group(1))\n",
    "        if \"minute\" in runtime:\n",
    "            minutes = int(re.search(r'(\\d+) minute', runtime).group(1))\n",
    "        total_minutes = hours * 60 + minutes\n",
    "        return total_minutes\n",
    "\n",
    "\n",
    "    def read_bronze_data(self):\n",
    "        schema = StructType([\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"release_date\", StringType(), True),\n",
    "            StructField(\"genre\", StringType(), True),\n",
    "            StructField(\"certificate\", StringType(), True),\n",
    "            StructField(\"vote_count\", StringType(), True),\n",
    "            StructField(\"runtime\", StringType(), True),\n",
    "            StructField(\"imdb_score\", StringType(), True),\n",
    "            StructField(\"director\", StringType(), True),\n",
    "            StructField(\"writter\", StringType(), True),\n",
    "            StructField(\"stars\", StringType(), True),\n",
    "            StructField(\"budget\", StringType(), True),\n",
    "            StructField(\"gross_global\", StringType(), True),\n",
    "            StructField(\"countries\", StringType(), True),\n",
    "            StructField(\"language\", StringType(), True),\n",
    "            StructField(\"locations\", StringType(), True),\n",
    "            StructField(\"company\", StringType(), True),\n",
    "            StructField(\"url\", StringType(), True),\n",
    "        ])\n",
    "        return ( spark.readStream\n",
    "                    .format('delta')\n",
    "                    .load(f\"{self.read_path}/bronze_delta_table\")\n",
    "                )\n",
    "\n",
    "    #get correct unit of money\n",
    "    def get_unit(self, money):\n",
    "        money = money.replace(',', '')\n",
    "        unit = ''.join(x for x in money if not x.isdigit())\n",
    "        return unit\n",
    "    #get number of money\n",
    "    def get_number(self, money):\n",
    "        money = money.replace(',', '')\n",
    "        number = ''.join(x for x in money if x.isdigit())\n",
    "        return float(number)\n",
    "    #convert to milion usd\n",
    "    def to_usd(self, money):\n",
    "        unit = self.get_unit(money)\n",
    "        number = self.get_number(money)\n",
    "        if unit == '$':\n",
    "            number = number\n",
    "        elif unit == '€':\n",
    "            number = number * 1.07\n",
    "        elif unit == '£':\n",
    "            number = number * 1.22\n",
    "        elif unit == '¥':\n",
    "            number = number * 0.0076\n",
    "        elif unit == '₩':\n",
    "            number = number * 0.0008\n",
    "        elif unit == '₹':\n",
    "            number = number * 0.012\n",
    "        elif unit == 'TRL\\xa0':\n",
    "            number = number * 0.053\n",
    "        elif unit == 'NOK\\xa0':\n",
    "            number = number * 0.1\n",
    "        elif unit == 'NOK\\xa0':\n",
    "            number = number * 0.1\n",
    "        elif unit == 'A$':\n",
    "            number = number * 0.69\n",
    "        elif unit == 'CA$':\n",
    "            number = number * 0.75\n",
    "        elif unit == 'DKK\\xa0':\n",
    "            number = number * 0.14\n",
    "        elif unit == 'SEK\\xa0':\n",
    "            number = number * 0.096\n",
    "        elif unit == 'MVR\\xa0':\n",
    "            number = number * 0.065\n",
    "        elif unit == 'NZ$':\n",
    "            number = number * 0.64\n",
    "        elif unit == 'PKR\\xa0':\n",
    "            number = number * 0.0044\n",
    "        elif unit == 'R$':\n",
    "            number = number * 0.19\n",
    "        elif unit == 'BDT\\xa0':\n",
    "            number = number * 0.0095\n",
    "        return number / 1000000\n",
    "\n",
    "\n",
    "\n",
    "    def transform_silver(self, bronze_df):\n",
    "\n",
    "        # pandas_df = bronze_df.alias('bronze_df').toPandas()\n",
    "\n",
    "        # pandas_df['gross_global'] = pandas_df['gross_global'].apply(lambda x: self.to_usd(x) if(type(x) == str) else x)\n",
    "        # pandas_df['budget'] = pandas_df['budget'].apply(lambda x: self.to_usd(x) if(type(x) == str) else x)\n",
    "        # bronze_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "        date_format_str=\"dd/MM/yyyy\"\n",
    "\n",
    "        silver_df = bronze_df.withColumn(\"release_date\", to_date(bronze_df[\"release_date\"], \"MMMM dd, yyyy\"))\n",
    "\n",
    "        silver_df = silver_df.withColumn(\"stars\", split(col(\"stars\"), \", \")) \\\n",
    "                              .withColumn(\"language\", split(col(\"language\"), \", \")) \\\n",
    "                              .withColumn(\"locations\", split(col(\"locations\"), \", \")) \\\n",
    "                              .withColumn(\"company\", split(col(\"company\"), \", \"))\n",
    "\n",
    "        #Convert vote_vcount\n",
    "        silver_df = silver_df.withColumn(\"vote_count\",\n",
    "                          when(col(\"vote_count\").contains(\"K\"),\n",
    "                                regexp_extract(col(\"vote_count\"), r'(\\d+)K', 1).cast(\"int\") * 1000)\n",
    "                          .otherwise(col(\"vote_count\").cast(\"int\")))\n",
    "\n",
    "        silver_df = silver_df.withColumn(\"runtime\", self.convert_to_minutes(silver_df[\"runtime\"]))\n",
    "        return silver_df\n",
    "\n",
    "\n",
    "    def upsert(self, silver_df, batch_id):\n",
    "        delta_table_path=f\"{self.write_path}/silver_delta_table\"\n",
    "        tmp_view_name=\"silver_df_temp_view\"\n",
    "        silver_df.createOrReplaceTempView(tmp_view_name)\n",
    "        merge_statement = f\"\"\"MERGE INTO delta.`{delta_table_path}` s\n",
    "                USING {tmp_view_name} t\n",
    "                ON s.url = t.url\n",
    "                WHEN MATCHED THEN\n",
    "                UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT *\n",
    "            \"\"\"\n",
    "        silver_df._jdf.sparkSession().sql(merge_statement)\n",
    "\n",
    "    def append_silver_data(self,silver_df,trigger):\n",
    "        sQuery = (silver_df.writeStream\n",
    "                    .queryName(\"silver-processing\")\n",
    "                    .format(\"delta\")\n",
    "                    .outputMode(\"update\")\n",
    "                    .foreachBatch(self.upsert)\n",
    "                    .option(\"checkpointLocation\",f\"{self.checkpoint_path}/silver_table_checkpoint\")\n",
    "        )\n",
    "\n",
    "        if(trigger=='batch'):\n",
    "            return ( sQuery.trigger(availableNow = True)\n",
    "                         .start(f\"{self.write_path}/silver_delta_table\"))\n",
    "        else:\n",
    "            return ( sQuery.trigger(processingTime = trigger)\n",
    "                         .start(f\"{self.write_path}/silver_delta_table\"))\n",
    "\n",
    "    def process(self,trigger='batch'):\n",
    "        print(f\"\\nStarting Silver Stream...\", end='')\n",
    "\n",
    "        # Read\n",
    "        bronze_df = self.read_bronze_data()\n",
    "\n",
    "        # Transform\n",
    "        silver_df = self.transform_silver(bronze_df)\n",
    "        #create_empty_delta_table(f\"{self.write_path}/silver_delta_table\",silver_df.schema)\n",
    "        def file_exists(dir):\n",
    "            try:\n",
    "                dbutils.fs.ls(dir)\n",
    "            except:\n",
    "                return False  \n",
    "            return True\n",
    "\n",
    "        if not file_exists(f\"{self.write_path}/silver_delta_table\"):\n",
    "            empty_df = spark.createDataFrame([], schema=silver_df.schema)\n",
    "            empty_df.write.format(\"delta\").mode(\"ignore\").save(f\"{self.write_path}/silver_delta_table\")\n",
    "\n",
    "        # Write\n",
    "        sQuery = self.append_silver_data(silver_df,trigger)\n",
    "        return sQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "632da073-7f91-45de-9a3d-97fdf11236f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active streaming queries.\n",
      "\n",
      "Starting Bronze Stream...\n",
      "\n",
      "Starting Silver Stream..."
     ]
    }
   ],
   "source": [
    "def stop_all_streaming_queries():\n",
    "    # Get the active streaming queries\n",
    "    active_queries = spark.streams.active\n",
    "\n",
    "    # Check if there are any active queries\n",
    "    if active_queries:\n",
    "        print(\"List of Active Streaming Queries:\")\n",
    "        for query in active_queries:\n",
    "            print(f\"Query Name: {query.name}, ID: {query.id}\")\n",
    "            query.stop()\n",
    "    else:\n",
    "        print(\"No active streaming queries.\")\n",
    "\n",
    "def main():\n",
    "    # spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "    bronze_base_dir='/mnt/bronze'\n",
    "    bronze_read_path='/mnt/bronze/bronze'\n",
    "\n",
    "\n",
    "    silver_base_dir='/mnt/silver'\n",
    "    silver_read_path=bronze_base_dir\n",
    "\n",
    "\n",
    "    bronze=BronzeBDS(bronze_read_path,bronze_base_dir,bronze_base_dir)\n",
    "    silver=SilverBDS(bronze_base_dir,silver_base_dir,silver_base_dir)\n",
    "\n",
    "    # Trigger as batch pipeline, change this parameter if you want stream pipeline\n",
    "    trigger='5 seconds'\n",
    "    bzQuery=bronze.process(trigger)\n",
    "    # bzQuery.awaitTermination()\n",
    "    slQuery=silver.process(trigger)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 394618973676285,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_function",
   "widgets": {}
  },
  "colab": {
   "authorship_tag": "ABX9TyOFDSIeTutJGktCYuVOB32f",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
